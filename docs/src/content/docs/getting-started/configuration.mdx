---
title: Configuration
description: Configure the essential settings for the synchronous scraping profile.
---

import { Aside } from "@astrojs/starlight/components";

To run the synchronous scraping profile, you need to provide the necessary connection details for the database and your BrightData Web Unlocker credentials.

All configuration is managed through environment variables, which are loaded from a `.env` file in the root directory of the project.

## Creating the `.env` File

In the root directory of the `streeteasy-scraper-monorepo` project, create a new file named `.env`. You can copy the contents of the `.env.example` file as a starting point:

```bash
cp .env.example .env
```

Now, you will edit the `.env` file to fill in the required values.

## Essential Environment Variables for Sync Profile

For the synchronous scraping profile, you need to configure the following environment variables in your `.env` file:

### Database Configuration

These variables are used to connect to the PostgreSQL database, which stores the addresses to scrape and the scraped data. The Docker Compose configuration already sets up an instance for you locally so you can use the following values:

```dotenv
POSTGRES_USER=user
POSTGRES_PASSWORD=password
POSTGRES_DB=user
POSTGRES_HOST=db
POSTGRES_PORT=5432
```

### BrightData Web Unlocker Credentials

These variables are used by the synchronous scraper to authenticate with BrightData's Web Unlocker service and route requests through their network.

- `BRIGHTDATA_USERNAME`: Your BrightData Web Unlocker username.
- `BRIGHTDATA_PASSWORD`: Your BrightData Web Unlocker password.
- `BRIGHTDATA_HOST`: The hostname provided by BrightData for the Web Unlocker proxy (e.g., brd.superproxy.io).

```dotenv
BRIGHTDATA_USERNAME=your_brightdata_username
BRIGHTDATA_PASSWORD=your_brightdata_password
BRIGHTDATA_HOST=brd.superproxy.io
```

<Aside type="note">
  The `BRIGHTDATA_API_TOKEN`, `BRIGHTDATA_CUSTOMER_ID`, `BRIGHTDATA_ZONE`, and
  `CLOUDFLARE_TUNNEL_TOKEN` variables in the `.env.example` file are not
  required for the synchronous scraping profile and can be left empty for now.
</Aside>

## `BOROUGHS_TO_KEEP` (Optional)

The `address_scraper` application uses the `BOROUGHS_TO_KEEP` environment variable to filter which addresses are saved to the database from the nyc.gov list.

- `BOROUGHS_TO_KEEP`: A comma-separated string of borough names you want to include. For example, to include only Manhattan and Brooklyn, you would set `BOROUGHS_TO_KEEP=MANHATTAN,BROOKLYN`. The default is `MANHATTAN`.

```dotenv
BOROUGHS_TO_KEEP=MANHATTAN
```

<Aside type="note">
  Any configurations to `address_scraper` doesn't matter too much because
  there's a limit of 500 addresses to save to the database. If you want to
  remove this limit, it's at
  [apps/address_scraper/scripts/run_address_scraper.py](https://github.com/nedhmn/streeteasy-scraper-monorepo/blob/main/apps/address_scraper/scripts/run_address_scraper.py)
  line 35.
</Aside>

Once you have filled in the required environment variables in your `.env` file, you are ready to run the synchronous scraping profile using Docker Compose.
